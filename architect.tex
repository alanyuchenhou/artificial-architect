\documentclass[12pt]{article}
\renewcommand*{\familydefault}{\sfdefault}
\usepackage{listings}
\usepackage[usenames,dvipsnames,svgnames,table]{xcolor}
%% \usepackage{color}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{fullpage}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{cite}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[section]
\begin{document}
\lstset{
  breaklines=true,
  commentstyle=\color{red},
  stringstyle=\color{orange},
  identifierstyle=\color{blue}
  keywordstyle=\color{violet},
  frame=single,
  language=Python}
\title{Architect projcet: on-chip network design automation}
\author{Yuchen Hou}
\maketitle

\begin{abstract}
  This project introduces machine learning enhanced on-chip network architecture design automation. It demonstrates a basic framework for the task. The framework is clarified with a concrete example of network topology optimization. A few experiments are conducted and summarize. The current experimental results have not yet provided evidences to prove this framework is indeed successful. The deficiency of this project and future work to address these issues are also discussed.
\end{abstract}

\section{Introduction}
\subsection{Motivation and challenges}
The problem this project focuses on is on-chip network design automation. On-chip network has become increasingly prevalent as the communication system in modern chip designs. A key in a good on-chip network is usually a good trade-off between many conflicting design features. An architect might be able to tell how good every feature of the design is, but even the most experienced architects can have only a vague idea about the best trade-off point. Along with the exponential growth of design complexity, design quality analysis and design feature trade-off become more difficult. Therefore, performing a large number simulations has become the primary method for many design optimization tasks. Although we have sophisticated simulators \cite{amoretti2014modeling} \cite{jiang2013detailed} to accurately assess design qualities, design processes still heavily depend on intellectual engagement of human. Also, simulation based design processes are time consuming because simulations are getting more expensive. Computer aided design tools we have now mostly deal with only low level design optimization, but not high level design optimization. This leads to a well known issue in chip design - productivity gap: design complexity keeps increasing rapidly, but designer productivity is lagged behind.
\subsection{Solution approach}
The solution proposed in this project is to construct a simple computer aided design tool for on chip network optimization. More specifically, a network design problem is formulated as a optimization problem and build a high level design tool with searching \cite{ganguly2011scalable} and learning ability to automatically optimize a network design, given design constraints and performance measure. The goal is that this tool can optimize a network autonomously, or at least significantly reduce the workload of human designers in a design process and therefore increase designer productivity.
\subsection{Results}
The current empirical results have not provide evidence suggesting that the tool is already effective enough for network optimization task. The fundamental problem is that the objective function learned so far cannot accurately predicts network design qualities, resulting in unsuccessful local search for optimum designs.

\section{Problem setup}
\subsection{Assumptions}
There are 3 assumptions on which this project is built:
\begin{enumerate}
\item On-chip network design can be successfully formulated as an optimization problem.
\item The quality of an arbitrary network design can be predicted by a model built by a learning algorithm.
\item The network simulator can accurately evaluate an arbitrary network design.
\end{enumerate}
\subsection{Task definition and formulation}
The typical network architecture design task for a human architect is an iterative process of the following steps:
\begin{enumerate}
\item{The architect analyzes the current design, and consider a few similar new designs}
\item{The architect assesses the qualities of new designs using his knowledge}
\item{The architect chooses a design with high quality and test it with a simulator}
\item{The architect examines the simulation report and improves his knowledge}
\item{The architect repeats the process until a good design is found}
\end{enumerate}
Compared to a human architect, an artificial architect(a computer program) probably fits this iterative process better with its sheer amount of computing resources a human cannot match. In order to construct this program, we can formulate the task into an optimization process, as described below:
\begin{description}
  \item[Network design problem] Optimization problem;
  \item[Fundamental design approach] Local search;
  \item[Knowledge] Objective function in local search, also the hypothesis function in learning(see below);
  \item[Knowledge improvement] Learning;
  \item[Knowledge based design quality analysis] Objective function based design quality estimation;
  \item[Simulation results] Training instances;
\end{description}
The essence of this tool is that it can learn an objective function that can wisely balance the trade-off between all those features and tells us what combination of them leads to the best quality design. It takes time to learn the objective functions because it demands experience acquired from real design examples.
\subsection{Network design specifications}
The network design can be specified by its topology adjacency matrix A, for example:
\begin{align*}
  \begin{pmatrix}
    1 & 0 & \cdots & 0 \\
    0 & 1 & \cdots & 1 \\
    \vdots & \vdots & \ddots & \vdots \\
    0 & 1 & \cdots & 1 \\
  \end{pmatrix} \\
\end{align*}
where A[i,j] specifies whether there is a link from node[i] to nod[j]: 1 stands for there is a direct link, and 0 indicates there is no direct link. The matrix is symmetric, as the network is an undirected graph. Figure \ref{fig:networks} shows some visualization examples for networks.
\begin{figure}[htb]
  \centering
  \begin{subfigure}
    {\includegraphics[width=0.4\textwidth]{graph3.png}}
  \end{subfigure}
  \begin{subfigure}
    {\includegraphics[width=0.4\textwidth]{graph4.png}}
  \end{subfigure}
  \rule{1\linewidth}{1pt}
  \caption{2 examples of 64 node networks with different link counts}
  \label{fig:networks}
\end{figure}
There are many other aspects in a network design beside network topology, which have not been taken into account yet and will be considered in future work.

\subsection{Network design features}
The features used to characterize the network are shown in Table \ref{tab:features}.
\begin{table}[htb]
  \centering
  \begin{tabularx}{\textwidth}{|l|X|} \hline
    name & description \\ \hline
    average degree & the average number of edges connected to a node \\ \hline
    %% link count & total number of links \\ \hline
    path length & average hop count in a path \\ \hline
    diameter & the maximum eccentricity of any node \\ \hline
    radius & the minimum eccentricity of any node \\ \hline
    %% link-cost & average time cost of a link \\ \hline
    %% route-cost & average time cost of a route \\ \hline
    %% buffer-size & link buffer size \\ \hline
    %% packet-size & size of packet in bytes \\ \hline
    %% routing & routing mechanism \\ \hline
    
    %% congestion control & congestion control mechanism \\ \hline
    %% link-count & average number of links connected to a router \\ \hline
    %% total-injection & total injection rate of the netwrok \\ \hline
    %% max-injection & maximum injection rate at a node \\ \hline
    %% link-bandwidth & average transmission rate of links \\ \hline
    %% route-bandwidth & average transimssion rate of routes \\ \hline
    %% double & total-deliver & total delivery rate of the network \\ \hline
    %% double & max-deliver & maximum delivery rate at a node \\ \hline
    %% double & total-packet & total number of packets delivered in the simulation \\ \hline
    %% double & congestion & maximum traffic at a node (incoming + outgoing) \\ \hline
    %% ???? & traffic distribution & indication of which nodes bear the most traffic \\ \hline
  \end{tabularx}
  \caption{The network design features}
  \label{tab:features}
\end{table}
\subsection{Network design quality}
There are many metrics that can be used to assess the quality of a network design, for example, latency, throughput, energy efficiency. In this project, only the energy efficiency of the final network is considered, which is evaluated by the network simulator.
%% \subsection{Metrics}
%% Theses metrics, shown in Table \ref{tab:metrics}, are evaluated by the external performance measures provided by the simulator, and also by the internal heuristic cost functions learned by the learning element. The performance measures and the heuristic cost functions should be in agreement as much as possible.
%% .
%% \begin{table}[htb]
%%   \centering
%%   \begin{tabularx}{\textwidth}{|l|X|} \hline
%%     name & description \\ \hline
%%     time-cost &  time consumed to finish the given workload \\ \hline
%%     energy-cost &  energy consumed to finish the given workload \\ \hline
%%     area-cost &  area required by the components on the chip \\ \hline
%%   \end{tabularx}
%%   \caption{The metrics evaluated by the performance measure and heuristic cost functions}
%%   \label{tab:metrics}
%% \end{table}

%% \subsection{Quality}
%% There should be another predefined evaluation function to assess the overall quality of a design based on its metrics. In the current setting, the quality evaluated only by energy consumption


\section{Task environment}
\subsection{Performance measure}
There are many ways to measure the performance of the architect program. A few obvious measures are listed below:
\begin{description}
  \item[Design quality] This is to be compared to energy efficiency of a few commonly adopted network topology.
  \item[Objective function accuracy] This is closely related to the design quality. More accurate objective function can naturally leads to better outcome of the local search for the final design. It is measured by testing accuracy of the hypothesis on a randomly generated testing data-set.
  \item[Search speed] This is the search step count for the local search to find a local optimum design. The faster the search can reach a local optimum, the better.
  \item[Long term improvement] During every local search process, the architect program invokes the simulator to accumulate more training data. This setup allows the learning element to improve the accuracy of the objective function to achieve long term improvement of the search. This long term improvement can be evaluated by comparing the final design quality of one local search to those of previous local searches.
\end{description}
The quality of the network design considered in this project is the energy efficiency. Other design metrics will be taken into account in future research.
\subsection{Environment}
The only significant element in the environment is the network simulator(which is developed by Professor Partha Pande's group at Washington State University). The simulator's characteristics are described in Table \ref{tab:environment}.
\begin{table}[htb]
  \centering
  \begin{tabularx}{\textwidth}{|l|l|X|} \hline
    type & name & description \\ \hline
    input & design specification &  network configuration files \\ \hline
    input & workload specification & simulation benchmark \\ \hline
    output & metrics & time, energy consumption, etc \\ \hline
  \end{tabularx}
  \caption{The characteristics of the simulator}
  \label{tab:environment}
\end{table}
The benchmark used in this project is only the bodytrack \cite{bienia2011benchmarking}, more benchmarks will be included in the future.
\subsection{Actuators and sensors}
They are responsible to write the design configuration files for the simulator, invoke the simulator to do experiments, read the simulation result file and extract design quality information from the simulation result.

\section{System design}
The system level design follows the learning agent framework from Russell and Norvig \cite{russell1995modern}.
%% \subsection{Percepts}
%% The percepts, shown in Table \ref{tab:percepts}, are directly aquired by the sensor from the enviroment as the raw information of a design.
%% \begin{table}[htb]
%%   \centering
%%   \begin{tabularx}{\textwidth}{|l|X|} \hline
%%     name & description \\ \hline
%%     node-location[i] & Cartesian coordinates (x,y) of node[i] \\ \hline
%%     traffic[i][j] & traffic from node[i] to node[j] in packets/second \\ \hline
%%     link-cost[i][j] & the time cost of the link from node[i] to node[j] \\ \hline
%%     buffer-size & link buffer size \\ \hline
%%     packet-size & size of packet in bytes \\ \hline
%%     routing & routing mechanism \\ \hline
%%     time-cost &  time consumed to finish the given workload \\ \hline
%%     energy-cost &  energy consumed to finish the given workload \\ \hline
%%     area-cost &  area required by the components on the chip \\ \hline
%%   \end{tabularx}
%%   \caption{The percepts received by the sensor}
%%   \label{tab:percepts}
%% \end{table}
\subsection{Critic}
It analyzes the design and constructs the design features. The training data is stored in a file for the learning element. A training data instance will have the following format:
\begin{align*}
[training-example] &= [quality] [features]\\
[quality] &= [energy]\\
[features] &= [1:degree] [2:average-path-length] [3:diameter] [4:radius]
\end{align*}
Below are a few training-examples:\\
2.86818e-05 1:6 2:2.45337301587 3:4 4:3\\
2.71615e-05 1:6 2:2.46974206349 3:4 4:3\\
2.86003e-05 1:6 2:2.48859126984 3:4 4:3\\
2.76535e-05 1:6 2:2.48015873016 3:4 4:3\\
2.73489e-05 1:5 2:2.70138888889 3:4 4:4\\
2.86277e-05 1:4 2:3.29216269841 3:6 4:4\\
\subsection{Learning element}
I use a support vector machine \cite{joachims2006training} in this project. The hypothesis this SVM needs to learn is the following function: for any network design, given the design features, the function predicts its energy consumption.
\begin{align*}
  design.energy &= hypothesis(design.features)
\end{align*}
However, as the SVM in use is not a regression learner, the exact prediction value does not have absolute meaning. But that does not affect its purpose of guiding the search, because it still gives predictions and instances with higher energy values gets higher predictions, all it needs to do is to distinguish the relative relation between different design's energy consumption.
\subsection{Performance element}
It carries out the local search using the objective function. In a normal hill climbing, higher quality designs should have higher values/scores. So a natual choice to meet the goal of reducing energy consumption is to the the objective as the negation of energy consumption
\begin{align*}
  design.score
  &= objective(design.features)\\
  &= - hypothesis(design.features)
\end{align*}
\section{Solution approach: optimization process}
The network optimization process is formulated as a local search using the framework from Russell and Norvig \cite{russell1995modern}.
\subsection{Naive approach: basic hill climbing}
The naive approach has only enough capability to finish a single search. It has many drawbacks, for example, prone to getting stuck at a local optimum, no long term improvement, no quality guarantee. It is merely a simple starting point. The specifics in the search are described in this section.
\begin{description}
\item[State]
Every search state is a network design, an undirected graph.
\item[Value]
The value of every state is the design quality estimated by the objective function. The objective function is a model built by the SVM. The parameters of the objective function are the network features, which are calculated using standard graph algorithms.
\item[Actions]
For every design, the agent applies a list of actions to it. Every action changes the connectivity within a unique cluster of size n (n number of nodes) to generate a successor design. For example, if the size of a cluster n is 4, the action is to change the connectivity within the 4 nodes in the cluster. Specifically, the action does the following modifications in that cluster: if there is a link between 2 nodes, the action removes that link; if there is no link between 2 nodes, the action adds a link between the 2 nodes. Every possible k-combination of n nodes correspond an action.
\item[Neighborhood]
The above actions lead the search from the current state to a number of neighbor states. All these neighbor states compose a neighborhood. The cluster size n can be considered as the radius of the neighborhood. Large n allows a large actions and therefore large neighborhood. The number of neighbors(also the number of actions) in a neighborhood of radius k is therefore
\begin{align*}
  C(n,k) = \frac{n!}{k!(n-k)!}
\end{align*}
\end{description}
\subsection{Refinement: random restart, learning and feedback}
Random restart is a common technique to reduce the issue of stuck-at local optima. learning and feedback is not typically found in a local search, as usually the objective function is assumed to be predefined and accurate. In this project, the initial objective function is learned from a limited data-set consisting of 1K instances of random network designs. Currently the accuracy of this objective function (and also the local search based on it) is not ideal. To address the problem of inaccurate objective function, learning is incorporated into the search process. At every search step, instead of moving directly into the next state, the agent invokes the simulator to experimentally evaluate the quality of the next design it has chosen. Then it adds information provided by this experiment(the quality and features of this next design) as a new instance into the training data-set and relearns the objective function after each search cycle. This approach has these advantages:
\begin{enumerate}
  \item As the search process enters new search regions, the model can be improved as new simulations can provide new training instances. Therefore, the search becomes more effective, as the model is adapted to the new environment continuously. In this way, the requirement for the previous learning process is also reduced: the model does not have to have good performance in the whole search space before the search starts.
  \item Learning is made more effective, too. The previous learning process is randomly conducted, as there are no substantial learning goals. But now there are. As the search is guided by the the model, so is the learning. It is likely that the search is moving towards more promising search region, where designs are better. So essentially the learner puts more effort on more promising designs. That might be a good learning goal: to spend more effort to learn how good designs are like.
  \item There will be more precise evaluation for the designs generated during the search. Without feedback from the simulator, there can be no proof that the design quality indeed improves along with the search. More interestingly, we will be able to depict and compare the design qualities predicted by the model and assessed by the simulator. This might be helpful for system tuning the improvement of both learning and searching.
\end{enumerate}
%% \subsection{Design evaluation}
%% The agent evaluate the reference quality of this design by weighted averaging the metrics returned from the simulator after a experiment.
%% \begin{align*}
%%   reference.quality = c1 \cdot time\_cost + c2 \cdot energy\_cost + c3 \cdot area\_cost
%% \end{align*}
%% where c1, c2 and c3 are negative constants.
%% A few design evaluation examples is below:
%% \begin{align*}
%%   c1 &= -973 \\
%%   c2 &= -65.4 \\
%%   c3 &= -34.6 \\
%%   ...\\
%%   reference.quality[36] &= -937 * 87.4 + -65.4 * 8.3 + -34.6 * 2.2 = -82512.74\\
%%   reference.quality[37] &= -937 * 37.4 + -65.4 * 4.3 + -34.6 * 1.2 = -35366.54\\
%%   reference.quality[38] &= -937 * 87.4 + -69.4 * 4.3 + -34.6 * 1.2 = -82233.74\\
%%   ...\\
%% \end{align*}

%% \lstinputlisting[firstline=43]{swNoCsim/swNoCsim/architect.py}

\section{Experiments and results}
\subsection{Basic hill climbing}
In this basic test is a single hill climbing search. It started from a randomly generated design and reached a local optimum. The neighborhood radius in this test is 2. The design features and scores were logged in every search step, which is shown in Figure \ref{fig:trace}. This result suggests the searching is operational - under the guidance of the objective function, the search finds designs with better and better design scores.
\begin{figure}[htb]
    \centering
    \begin{subfigure}
      {\includegraphics[width=\textwidth]{trace-2014-12-18-14-48-05.png}}
    \end{subfigure}
    \rule{\linewidth}{1pt}
    \caption{The basic search process. The score is calculated with the model learned from a previously obtained data set.}
    \label{fig:trace}
\end{figure}
However, There are 2 obvious weaknesses in this experiment:
\begin{enumerate}
  \item The design score predicted by the objective function is not a reliable justification of the design quality. Only the energy consumption predicted by the simulator is. If the objective function prediction deviates from the simulator prediction too much, the search can never find a optimum design.
  \item The degree has abnormal trend after 60 steps. Instead of increasing, it started to decrease, and even all the way to a very low level of 2. This phenomenon is against our experience for network design, and is a good sign that the model previously learned does not perform well.
\end{enumerate}
\subsection{Hill climbing with random restart, learning and feedback}
The test after the first refinement confirmed the problem exposed in the previous test: the model does not perform well, as indicated by the feedback from the simulator. The neighborhood radius is still 2 in this test. Figure \ref{fig:refined} shows the search history in the 2nd experiment, after random restart, learning and feedback are applied to the search. This test is still in progress, but the current result is already quite dramatic.
\begin{figure}[htb]
    \centering
    \begin{subfigure}
      {\includegraphics[width=\textwidth]{trace.png}}
    \end{subfigure}
    \rule{\linewidth}{1pt}
    \caption{The 2nd search process. The new energy consumption is obtained from the simulator's feedback.}
    \label{fig:refined}
\end{figure}
All the data trace are discontinuous because of random restart. Every time the search reaches a local optimum, the search restarts and all the features, score and energy jump to new levels. It is not clear whether learning during the search process has effectively improved the search performance; the subsequent search process may provide more insight to this question. The fluctuation of the energy does decrease gradually, but this is not a strong evidence that the this results from more accurate prediction and hence the learning process.  Within every search cycle, the features and scores still behave as in the previous test. However, the trend of the energy is far from what I expected. In the ideal situation, the energy should certainly decrease monotonically, as that is the goal of the search. Practically speaking, the prediction given by the model(the score) will have some deviation from the real value(energy), so the energy is supposed to decrease with fluctuation. One can argue that the energy does 'decrease with fluctuation' in every search cycle according to the data provided in this test, but this argument is weak because the magnitude of the energy fluctuation is even larger than the overall energy decrements. If the assumption that the simulation is accurate holds, the explanation should be the network energy is very sensitive to minor changes in the network topology. Further more, I can infer that minor changes in the topology can radically change some features not captured in the current learning process. It is also possible that those design features are not closely related to network topology. This analysis need more knowledge about network design.
\section{Conclusion and future work}
The current architect program is not yet able to solve the problem it is designed to solve due to inaccurate objective function used in the local search. More advanced search technique may improve the search result. However, an accurate objective function and successful learning mechanism are much more likely to fundamentally solve the current problem.
\subsection{More expressive design features}
This is an important way to improve the design quality in the long run. I will record the states of the searches and their features, qualities predicted by the objective function and also assessed by the simulator. Then I need to identify the search process where model predictions deviate from simulation assessment, and also characterize the common bad features of these designs which the learning has not taken into account yet, using domain knowledge specific for network on chip design. The I can add these new features that reflect the drawbacks of these networks into the learning problem. In this way, the objective function will relate these features to design qualities through learning and prevent future searches from getting into these bad states.
\subsection{New learning algorithm}
An learning algorithm that fits online learning is needed, because learning process is getting longer as training data-set increases. By 2014-12-19, the training dataset has exceeded 1.5K training instances; every new learning takes more than 1 day. Current learning frequency is once every search cycle, but it is still too expensive. Also I will find a ML library to replace the current standalone learning program to avoid system calls and file read / write operations, to speedup the learning and searching.
\subsection{Learning algorithm tuning}
More suitable kernels and parameters may help the SVM used in this project to fit the learning task better. However, systematic and effective tuning is itself a search problem - I am essentially searching for a optimum SVM setup. I will need more knowledge to carry out this search.
\subsection{Model oriented learning}
One of the assumption of this project is that the quality of an arbitrary design can be accurately predicted by a model built by a learning algorithm. It is not clear if this assumption depends on any conditions. It is possible that the tool has to learn a much more complex model, because its purpose is to serve as an analytical replacement of the simulator(which is very complex) in situations where accuracy requirement is low. This model will be less accurate than the the simulator, but also has to be much less expensive to use. This work depends on a good knowledge on both the simulator design and learning technique.
\subsection{Large neighborhood and first-choice hill climbing}
Big neighborhood provide more opportunities for uphill moves, but they are usually way too slow because there are too many neighbors to examine. For example, even if the radius of the neighborhood increases to only 3, the climbing speed can drop to 2 uphill moves per hour. In future work, I will examine the possibility of combining large neighborhood with first-choice hill climbing, as first-choice hill climbing can avoid exhaustive examination of the entire neighborhood and its performance penalty. However, this approach with a very large neighborhood has a drawback. The hill climbing might have very big actions so that it becomes more like a series of jumps in the search space, instead of climbing a hill step by step, as in a typical local search.
\subsection{Small neighborhood and simulated annealing}
Simulated annealing can also address the issue of getting stuck at local optima, which I should also try in the future. As the workload of simulated annealing is even higher than a normal hill climbing search,only small neighborhood should be consider in this scenario. The drawback of this approach is not clear yet.
\subsection{Adaptive searching}
Instead of fix the hill climbing methods, the agent might adapt its searching algorithm to the specific situation it is in. For example, at the beginning of the search, it can use first-choice hill climbing with large neighborhood to rapidly approach a good neighborhood; then it can switch to simulated annealing with small neighborhood to conduct more thorough search around that neighborhood.
\subsection{Neighbor filtering}
I will consider to find high performance filter to discard the neighbors with bad qualities before they are examined by the objective function to reduce the workload of the examination. This filter must be less expensive than the objective function.
\subsection{Program profiling}
Currently, program run time is not a the primary focus, but the prolonged search process does have an negative effect to the project. Profiling is worth considering in the future.

\bibliographystyle{plain}
\bibliography{architect}
\end{document}
